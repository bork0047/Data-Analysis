{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bork0047/Data-Analysis/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D0%B5_%D0%BD%D0%B0_TextAnalysis_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb91vLHqYu4w"
      },
      "source": [
        "# Text Analytics Part 2: Applying Machine Learning To Text Classification\n",
        "\n",
        "This document covers the use of text preprocessing \n",
        "pipelines and parameter grids for text analysis. It also demonstrates how to create your own transformer to include in a pipeline and concludes with a comparative analysis of multiple machine learning algorithms on a text analysis problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpUIy-0n_3c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#for nlp\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#text vectorisation\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "#import method releated to evaluation\n",
        "from sklearn import model_selection\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "\n",
        "#classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#for graphs\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxchYEuWn7WW"
      },
      "source": [
        "#1. Text classification with IMDb movie review data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeE88GT4n7WW"
      },
      "source": [
        "## Obtaining the IMDb movie review dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WximTstn7WW"
      },
      "source": [
        "The IMDB movie review set can be downloaded from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "We have already done this and extracted the csv file which you can access from our github.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_7zRkZzmlJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "688ef38b-ab60-421f-b10b-20dfbf187206"
      },
      "source": [
        "data_file = \"https://raw.githubusercontent.com/nirmalie/CM4107/main/movie_data_cat.csv\"\n",
        "# class_index = 1 # on inspection of the csv file we see that the class appears in 1st position\n",
        "df = pd.read_csv(data_file,  encoding='utf-8')\n",
        "print('Shape:', df.shape)\n",
        "\n",
        "df.head(10)  # top 10 rows"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (50000, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...       pos\n",
              "1  OK... so... I really like Kris Kristofferson a...       neg\n",
              "2  ***SPOILER*** Do not read this, if you think a...       neg\n",
              "3  hi for all the people who have seen this wonde...       pos\n",
              "4  I recently bought the DVD, forgetting just how...       neg\n",
              "5  Leave it to Braik to put on a good show. Final...       pos\n",
              "6  Nathan Detroit (Frank Sinatra) is the manager ...       pos\n",
              "7  To understand \"Crash Course\" in the right cont...       pos\n",
              "8  I've been impressed with Chavez's stance again...       pos\n",
              "9  This movie is directed by Renny Harlin the fin...       pos"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a62f8fa0-7129-41fe-be8f-02fad87a2fdd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I've been impressed with Chavez's stance again...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a62f8fa0-7129-41fe-be8f-02fad87a2fdd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a62f8fa0-7129-41fe-be8f-02fad87a2fdd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a62f8fa0-7129-41fe-be8f-02fad87a2fdd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_uoHygSn7WW"
      },
      "source": [
        "## Preprocessing the movie dataset into more convenient format\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5NUcj4zn7WX"
      },
      "source": [
        "Since the sentiment column happens to be categorical we can map the \"pos\" and \"neg\" classes to 0 and 1 integers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGRPPLxn7WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "10a273ef-d32b-40fc-cf4c-9735c1c6fff3"
      },
      "source": [
        "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['sentiment']))}\n",
        "\n",
        "print(\"class_mapping:\" , class_mapping)\n",
        "\n",
        "#use the mapping dictionary to transform the class labels into integers\n",
        "#the data frame series takes the mapping and applies it to each element in the series\n",
        "#this mapping here acts like a replacement / subsititution (i.e. replace neg with 0 etc.)\n",
        "\n",
        "df['sentiment'] = df['sentiment'].map(class_mapping)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_mapping: {'neg': 0, 'pos': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0\n",
              "5  Leave it to Braik to put on a good show. Final...          1\n",
              "6  Nathan Detroit (Frank Sinatra) is the manager ...          1\n",
              "7  To understand \"Crash Course\" in the right cont...          1\n",
              "8  I've been impressed with Chavez's stance again...          1\n",
              "9  This movie is directed by Renny Harlin the fin...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-814c2e49-36a0-4987-9168-5b1fe4698c6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Nathan Detroit (Frank Sinatra) is the manager ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>To understand \"Crash Course\" in the right cont...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I've been impressed with Chavez's stance again...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>This movie is directed by Renny Harlin the fin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-814c2e49-36a0-4987-9168-5b1fe4698c6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-814c2e49-36a0-4987-9168-5b1fe4698c6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-814c2e49-36a0-4987-9168-5b1fe4698c6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjY21LvJn7WX"
      },
      "source": [
        "Now that the class column is as we need it for a classifier we next look at how to clean up the review text content. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aYZQhGan7WX"
      },
      "source": [
        "### Cleaning text data with Regular Expressions\n",
        "Execute the code below to view a specific review. \n",
        "You will notice that the text needs cleaned up e.g. due to html markup, punctuation and other non-letter chars. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN8WYOiMDkTY"
      },
      "source": [
        "#import regular expressions to clean up the text\n",
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text) # remove all html markup ; re.sub(A, B, C) will Replace A with B in the string C.\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons re.findall(A, B) | Matches all instances of an expression A in a string B and returns them in a list.\n",
        "    \n",
        "    # remove the non-word chars '[\\W]+'\n",
        "    # append the emoticons to end \n",
        "    # convert all to lowercase\n",
        "    # remove horizontal nose char for consistency so that :-) is the same as :)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', '')) \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4vVYhSGn7Wa"
      },
      "source": [
        "### Stopwords, Tokenisation and Stemming \n",
        "We need to down load the stopwords list from nltk.\n",
        "You can do that as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8CApRMVn7WZ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb1008e-13dc-416a-a0ba-a03e8efd5e34"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "#get the english stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "print(\"Number of stopwords:\", len(stop))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stopwords: 179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSCZm-ni-Ixl"
      },
      "source": [
        "Let's create  a method to stem each word / token contained in the piece of text. \n",
        "The text is first tokenised before stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r_yEJdQn7Wa"
      },
      "source": [
        "#get the english stemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tokenizer(text):\n",
        "       return text.split()\n",
        " \n",
        "\n",
        "def tokenizer_stemmer(text):\n",
        "    return [stemmer.stem(word) for word in tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxEvnB2nG9Z9"
      },
      "source": [
        "### Get the Training and Testing splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXwJPKWKn7Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf481330-dc9b-4e3a-ade1-b3568958eee8"
      },
      "source": [
        "#get a small sample from the 50K data\n",
        "small_df= df.sample(frac=0.01, replace=False, random_state=1)\n",
        "#Strip HTML and punctuation to speed up text processing\n",
        "small_df['review'] = small_df['review'].apply(preprocessor)\n",
        "\n",
        "#get the X and y parts of the data\n",
        "X = small_df.loc[:, 'review'].values\n",
        "y = small_df.loc[:, 'sentiment'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                          random_state=42,\n",
        "                                                          test_size=0.30, # lets use 30% for testing\n",
        "                                                          stratify=y)\n",
        "print(\"Sample of Training data:\", X_train.shape)\n",
        "print(\"Sample of Test data:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of Training data: (350,)\n",
            "Sample of Test data: (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGhSn9TQn7Wb"
      },
      "source": [
        "### Convert dataset to vector representation\n",
        "\n",
        "We use scikit-learn's `TfidfVectorizer` to convert the dataset to TF-IDF features for use by a machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjSD7YcksxxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283bcae8-85dc-4b0c-c0c1-b01c0b981f0c"
      },
      "source": [
        "# initialise the vectorisers \n",
        "vectorizer_imdb = TfidfVectorizer(max_features=500)\n",
        "vectorizer_imdb.fit(X) # Learn a vocabulary dictionary of all tokens in the raw documents.\n",
        "\n",
        "# vectorise the train and test data\n",
        "X_train_vec0 = vectorizer_imdb.transform(X_train)# transform to features for input into our classifier\n",
        "X_test_vec0 = vectorizer_imdb.transform(X_test)\n",
        "print(\"Sample of Training data:\", X_train_vec0.shape)\n",
        "print(\"Sample of Test data:\", X_test_vec0.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of Training data: (350, 500)\n",
            "Sample of Test data: (150, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siK4Zwxkp0t"
      },
      "source": [
        "## <font color='red'>1.1 Exercise</font> Model for sentiment classification\n",
        "\n",
        "Train a `LogisticRegression` model with default parameters using the vectorised dataset. Determine the training accuracy and the test accuracy.\n",
        "Note: When you call `score` on classifiers like LogisticRegression, RandomForestClassifier, etc. the method computes the accuracy score by default (accuracy is #correct_preds / #all_preds). Therefore by default, the score method does not need the actual predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weUaXJr5lfNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff8a138-7c15-457f-ca72-73a9433648c8"
      },
      "source": [
        "## SOLUTION 1.1\n",
        "# Train model\n",
        "\n",
        "clf = LogisticRegression()# complete here to initialise the LogisticRegression classifier model\n",
        "\n",
        "clf.fit(X_train_vec0, y_train) # complete here to fit the classifier\n",
        "\n",
        "# Training accuracy\n",
        "train_accuracy = clf.score(X_train_vec0, y_train)# complete here to get the train score from the classifier\n",
        "\n",
        "# Test accuracy\n",
        "test_accuracy = clf.score(X_test_vec0, y_test)# complete here to get the test score from the classifier\n",
        "\n",
        "print(\"train accuracy:\", train_accuracy)\n",
        "print(\"test accuracy:\", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy: 0.9285714285714286\n",
            "test accuracy: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D64GiRqxdnP"
      },
      "source": [
        "#2. Building Pipelines and and using Grid Searches\n",
        "In the above example we used the TF-IDF vectoriser to transform both the train and test data before fitting a classifier for prediction. \n",
        "This pipeline of transformation steps and the final prediction can be carried out by setting up a pipeline.\n",
        "\n",
        "Instead of using the transformed vectors of X_train and Y_train ; we can use the original train and test which contained the text data i.e. X_train and X_test. \n",
        "These can then be sent through the transformation pipeline steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Form the train test splits"
      ],
      "metadata": {
        "id": "A5tzsdwpFn9b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ20ofG0IRQ-"
      },
      "source": [
        "small_df= df.sample(frac=0.01, replace=False, random_state=1)\n",
        "X = small_df.loc[:, 'review'].values\n",
        "y = small_df.loc[:, 'sentiment'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                          random_state=42,\n",
        "                                                          test_size=0.30, # lets use 30% for testing\n",
        "                                                          stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcvkrctzNLrH"
      },
      "source": [
        "## Build a Custom Transformer to preprocess the text (before vectorising)\n",
        "\n",
        "Text clean-up to remove non-word characters (including HTML tags) from the text of movie reviews and convert to lowercase is one of the preprocessing steps that we used. This data preprocessing was achieved using our `preprocessor()`. Here, we convert the `preprocessor()` function to a tranformer to include in a pipeline using scikit-learn's FunctionTransformer. To demonstrate the use of parameters in the custom transformer, we include the ability to specify whether to convert the text to lowercase or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGvyvx5Mn7WY"
      },
      "source": [
        "#import sklearn's FunctionTransformer to create a transformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "def text_cleanup(X, lowercase):  #lowercase: True to convert to lowercase or False to leave case as they are\n",
        "    # print('in text_cleanup...')\n",
        "    # print('lowercase:', lowercase)\n",
        "    XX = np.copy(X)  # get copy of the data\n",
        "    for index, text in enumerate(XX):\n",
        "        text = re.sub('<[^>]*>', '', text) # remove all html markup ; re.sub(A, B, C) will Replace A with B in the string C.\n",
        "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # findall the emoticons re.findall(A, B) | Matches all instances of an expression A in a string B and returns them in a list.\n",
        "        \n",
        "        # remove the non-word chars '[\\W]+'\n",
        "        # append the emoticons to end \n",
        "        # conditionally convert all to lowercase\n",
        "        # remove nose char for consistency\n",
        "        if lowercase:\n",
        "            text = text.lower()\n",
        "        text = (re.sub('[\\W]+', ' ', text) +\n",
        "                ' '.join(emoticons).replace('-', ''))\n",
        "        XX[index] = text\n",
        "\n",
        "    return XX\n",
        "\n",
        "#creating text_cleanup object using FunctionTransformer\n",
        "text_cleanup_preprocessor = FunctionTransformer(text_cleanup, kw_args={'lowercase': True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxrT9ZJtRjAi"
      },
      "source": [
        "## Specify and execute a Pipeline\n",
        "\n",
        "Next, we include the custom transformer in the pipeline of transforms. We also include feature selection using chi-square."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nSe4sqxcHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e07ff03-4843-4b17-f8c5-1de88355e7b9"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "                        text_cleanup_preprocessor,  # custom transformer for text cleanup\n",
        "                        TfidfVectorizer(), # include the vectoriser\n",
        "                        SelectKBest(chi2, k = 20),  # feature selection with chi-square using k best features\n",
        "                        LogisticRegression(random_state=1, solver='lbfgs')\n",
        "                        )\n",
        "\n",
        "# SOLUTION here fit, predict and show the classification report\n",
        "\n",
        "# add code here to fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# predict \n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# print just the test accuracy \n",
        "print('Test accuracy: %s' % pipeline.score(X_test, y_test))\n",
        "\n",
        "# generate a classification report\n",
        "class_labels = [k for k, v in class_mapping.items()]\n",
        "\n",
        "#print(class_labels)\n",
        "#print(class_mapping)\n",
        "classification_report(y_test, y_pred, target_names=class_labels)\n",
        "# add code here to get the predictions for the test set i.e. for y_pred\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.78      0.74      0.76        76\n",
            "         pos       0.74      0.78      0.76        74\n",
            "\n",
            "    accuracy                           0.76       150\n",
            "   macro avg       0.76      0.76      0.76       150\n",
            "weighted avg       0.76      0.76      0.76       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdE9mOJqsyc1"
      },
      "source": [
        "## Setting up Grid Search\n",
        "In the previous cell we calculated results based on a single test-train split. \n",
        "Ideally we want to do this using cross-validation. For this purpose we can use GridSearchCV.\n",
        "Details of the many possible TfidfVectorizer parameters (see the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwpLQBpnn7Wb"
      },
      "source": [
        "param_grid0 = [{\n",
        "    'clf__solver': [('lbfgs'), ('liblinear')],\n",
        "    'vect__ngram_range': [(1,1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
        "    'vect__tokenizer': [tokenizer_stemmer], # use a tokeniser and the stemmer \n",
        "    'vect__stop_words': [stop, None], # try with and without the stop dictionary\n",
        "    'tcp__kw_args': [{'lowercase': True}, {'lowercase': False}]  # case insensitive or not parameters for our custom transformer\n",
        "  }]\n",
        "\n",
        "#Sentiment classification with a neural net\n",
        "pipe_tfidf = Pipeline([\n",
        "                  ('tcp', text_cleanup_preprocessor),\n",
        "                  ('vect', TfidfVectorizer()),\n",
        "                  ('chi', SelectKBest(chi2, k = 20)),\n",
        "                  ('clf', LogisticRegression())\n",
        "  ])\n",
        "                \n",
        "gs_sentiment_classification = GridSearchCV(pipe_tfidf, param_grid0,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ow5o-QTn7Wc"
      },
      "source": [
        "#### <font color='red'>Warning</font>\n",
        "**Important Note about the running time**\n",
        "\n",
        "Executing the following code cell **may take up to 30-60 min** depending on your machine, since based on the parameter grid we defined, there are many models to fit.\n",
        "\n",
        "If you do not wish to wait so long, you could reduce the size of the dataset by decreasing the number of training samples, for example, as follows:\n",
        "\n",
        "    X_train = df.loc[:2500, 'review'].values\n",
        "    y_train = df.loc[:2500, 'sentiment'].values\n",
        "    \n",
        "However, note that decreasing the training set size to such a small number will likely result in poorly performing models. Alternatively, you can delete parameters from the grid above to reduce the number of models to fit -- for example, by using the following:\n",
        "\n",
        "    param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                   'vect__stop_words': [stop, None],\n",
        "                   'vect__tokenizer': [tokenizer],\n",
        "                  ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG9ADf7yn7Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab421414-a520-4fe0-fb1b-00580c87f0e6"
      },
      "source": [
        "gs_sentiment_classification.fit(X_train, y_train) # Tfidf vectoriser will transform so send the X_train not the vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('tcp',\n",
              "                                        FunctionTransformer(func=<function text_cleanup at 0x7fab91132c20>,\n",
              "                                                            kw_args={'lowercase': True})),\n",
              "                                       ('vect', TfidfVectorizer()),\n",
              "                                       ('chi',\n",
              "                                        SelectKBest(k=20,\n",
              "                                                    score_func=<function chi2 at 0x7fab90fc1830>)),\n",
              "                                       ('clf', LogisticRegression())]),\n",
              "             n_jobs=1,\n",
              "             param_grid=[{'clf__solver': ['lbfgs', 'liblinear'],\n",
              "                          'tcp__kw_args': [{'lowe...\n",
              "                          'vect__ngram_range': [(1, 1)],\n",
              "                          'vect__stop_words': [{'a', 'about', 'above', 'after',\n",
              "                                                'again', 'against', 'ain',\n",
              "                                                'all', 'am', 'an', 'and', 'any',\n",
              "                                                'are', 'aren', \"aren't\", 'as',\n",
              "                                                'at', 'be', 'because', 'been',\n",
              "                                                'before', 'being', 'below',\n",
              "                                                'between', 'both', 'but', 'by',\n",
              "                                                'can', 'couldn', \"couldn't\", ...},\n",
              "                                               None],\n",
              "                          'vect__tokenizer': [<function tokenizer_stemmer at 0x7fab91deb830>]}],\n",
              "             scoring='accuracy', verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DDJOSgn_3t"
      },
      "source": [
        "## <font color='red'>2.1 Exercise</font> Grid search for text classification\n",
        "1. Once the gridsearchCV is complete; access the best_params_ and print it. Hint: refer to evaluation lab part 2 on using best_params_\n",
        "2. use best_score_ to access the accuracy for the best parameter combination on the train set\n",
        "3. use the best_estimator_ to make predictions on the unseen text data i.e. text_test. You can use gs_tfidf.best_estimator_.score(text_test, y_test) to calculate the accuracy on test.\n",
        "4. Use classification_report() to provides a summary of performance on unseen test data. You need to get the predictions for the test data for this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKfai4Nn7Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e372de70-ae1a-4c61-ce3e-16e976b0dc6d"
      },
      "source": [
        "## SOLUTION 2.1\n",
        "# 1. Best parameters\n",
        "print(gs_sentiment_classification.best_params_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clf__solver': 'lbfgs', 'tcp__kw_args': {'lowercase': True}, 'vect__ngram_range': (1, 1), 'vect__stop_words': {\"don't\", 'below', 'doing', 'over', 'that', 'haven', 'above', 'be', 'these', 'their', 'own', 'few', 'not', 'are', 'such', 'but', 'on', 'those', \"wouldn't\", 'with', 'if', \"mightn't\", 'am', 'didn', 'through', 'won', 'out', \"you'll\", 'while', \"hasn't\", 'as', 'at', 'after', 'just', 'by', 'against', 'o', 'hasn', \"aren't\", 'all', \"she's\", 'once', 'where', 'most', 'themselves', 'again', 'yourself', 'and', 'me', 'between', \"that'll\", 'y', 'a', 'more', 'into', 'myself', 'd', \"haven't\", 'too', 'down', 'theirs', 'of', 'is', 'why', 'you', 'for', 'when', 'we', 'very', 'hers', \"couldn't\", 'or', 'weren', 'ourselves', 't', 'been', 'mightn', 'they', 'who', 'were', 'can', 'had', 'how', 'during', 'does', 'now', \"shouldn't\", 'so', 'the', 'yourselves', 'him', 'them', 'our', 'm', \"shan't\", 'shouldn', 'having', 'nor', \"you've\", 'wouldn', \"didn't\", 'under', 'don', 'to', 'some', 'aren', \"mustn't\", 'ma', 'doesn', 'hadn', 's', 'this', 'his', 'same', \"wasn't\", 'needn', \"should've\", \"doesn't\", 'll', 'only', 'which', 'further', 'here', 'each', 'any', 'in', 'have', 're', \"isn't\", 'was', \"hadn't\", 'did', 'himself', 'her', \"it's\", 'itself', 'from', 'off', 'couldn', 'until', 've', 'up', 'because', 'wasn', 'has', 'should', 'about', 'what', 'being', 'i', 'both', 'its', 'no', 'whom', \"weren't\", \"needn't\", 'she', 'herself', 'it', \"won't\", 'will', 'ours', 'ain', 'shan', 'yours', 'there', \"you're\", 'an', 'then', 'than', 'he', \"you'd\", 'my', 'your', 'before', 'isn', 'mustn', 'do', 'other'}, 'vect__tokenizer': <function tokenizer_stemmer at 0x7fab91deb830>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqg2KYt73-q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf58413-4bb6-405e-b9c4-78e285082b1a"
      },
      "source": [
        "# 2. Best score\n",
        "\n",
        "gs_sentiment_classification.best_score_ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7742857142857144"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaYYmTyXn7Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123cae65-ba77-4ccb-a5de-fedfb24ea165"
      },
      "source": [
        "# 3. Calculate the accuracy on the unseen text data using best_estimator_ and score\n",
        "\n",
        "clf = gs_sentiment_classification.best_estimator_ \n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15idY-G0pUsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7332a70d-e4b0-4d6e-ec1a-9d71854c103b"
      },
      "source": [
        "# 4. Performance summary on the test dataset using classification_report()\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.77      0.75      0.76        76\n",
            "         pos       0.75      0.77      0.76        74\n",
            "\n",
            "    accuracy                           0.76       150\n",
            "   macro avg       0.76      0.76      0.76       150\n",
            "weighted avg       0.76      0.76      0.76       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hNVSFDmn7Wc"
      },
      "source": [
        "####  GridSearchCV versus cross_val_score\n",
        "    \n",
        "Please note that **best_score_** is the average k-fold cross-validation score. I.e., if we have a `GridSearchCV` object with 5-fold cross-validation (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. \n",
        "In contrast cross_val_score will return the individual preformance values for each fold. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do9ChKZIiPNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8034331-0ac3-4e5e-95c9-bf512d2708ae"
      },
      "source": [
        "cross_val_score(gs_sentiment_classification, X_train, y_train, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81428571, 0.77142857, 0.82857143, 0.77142857, 0.68571429])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHXrXfg3n7Wc"
      },
      "source": [
        "## <font color='red'>2.2 Exercise</font> Exploring text classifier alternatives with grid search\n",
        "In the previous code you saw that having the stopword remover (i.e. stop) was better than None (not having stopword removal). You can now explore other alternatives. Change the param_grid0  values to explore the following:\n",
        "1.  Does having Tokenizer_stemmer result in better accuracy compared to just the tokenizer (with no stemmer)?\n",
        "2. What happens if you consider both unigrams and bigrams? Is there a benfit to analysing bigrams?\n",
        "3. Include comparisons for different tfidf parameter values such as min_df (e.g. 7, 10, etc.), max_df (e.g. 0.75, 0.95, etc.), max_features (e.g. 500, 1000, or None)\n",
        "4. How can you modify the classifier 'clf' to a different learner such as the \n",
        "NaiveBayes - MultinomialNB(alpha=1.0, class_prior=None) ; or any of the learners you have tried in the previous lab ; e.g.\n",
        "MLPClassifier  - MLPClassifier(activation='relu', solver='adam', max_iter=2000, hidden_layer_sizes=(100,))  SVC(kernel='linear', gamma=0.7, C=1.0)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOLUTION 2.2\n",
        "param_grid22 = [{\n",
        "    'clf__alpha': [(1.0)],\n",
        "    'clf__class_prior': [(None)],\n",
        "    'vect__ngram_range': [(1,1),(1,2)], #complete here to extract 2-grams of words in addition to the 1-grams (individual words)\n",
        "    'vect__tokenizer': [stemmer, tokenizer_stemmer], #complete here for a tokeniser and the stemmer \n",
        "    'vect__stop_words': [stop, None], # try with and without the stop dictionary\n",
        "    'vect__max_features': (500, 1000, None), #complete alternatives here for a vocabulary that only consider the top max_features ordered by term frequency\n",
        "    'vect__min_df': (7, 10), #complete here for min_df i.e. ignore terms that have a document frequency strictly lower than )\n",
        "    'vect__max_df': (0.75, 0.95), #ignore terms that have a document frequency strictly higher \n",
        "    'tcp__kw_args': [{'lowercase': True}, {'lowercase': False}]  # case insensitive or not parameters for our custom transformer\n",
        "  }]\n",
        "\n",
        "#Sentiment classification with a neural net\n",
        "pipe_tfidf22 = Pipeline([\n",
        "                  ('tcp', text_cleanup_preprocessor),\n",
        "                  ('vect', TfidfVectorizer()),\n",
        "                  ('chi', SelectKBest(chi2, k = 20)),\n",
        "                  ('clf', MultinomialNB()) # complete code here\n",
        "\n",
        "  ])\n",
        "                \n",
        "gs_sentiment_classification22 = GridSearchCV(pipe_tfidf22, param_grid22,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=1) \n",
        "\n"
      ],
      "metadata": {
        "id": "yGhI5WknqtzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_sentiment_classification22.fit(X_train, y_train)\n",
        "print('Best parameter set: %s ' % gs_sentiment_classification22.best_params_)\n",
        "print('CV Accuracy: %.3f' % gs_sentiment_classification22.best_score_)\n",
        "\n",
        "clf22 = gs_sentiment_classification22.best_estimator_\n",
        "print('Test Accuracy: %.3f' % clf22.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px7884witkQC",
        "outputId": "c2e28b03-ccc2-4637-fa17-2563f12f263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "480 fits failed out of a total of 960.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "480 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 390, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 355, in _fit\n",
            "    **fit_params_steps[name],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\", line 2077, in fit_transform\n",
            "    X = super().fit_transform(raw_documents)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\", line 1330, in fit_transform\n",
            "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
            "    for feature in analyze(doc):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\", line 115, in _analyze\n",
            "    doc = tokenizer(doc)\n",
            "TypeError: 'SnowballStemmer' object is not callable\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.68571429        nan 0.65714286        nan 0.68571429\n",
            "        nan 0.64571429        nan 0.68              nan 0.65714286\n",
            "        nan 0.68              nan 0.65428571        nan 0.69428571\n",
            "        nan 0.68857143        nan 0.69142857        nan 0.66285714\n",
            "        nan 0.68857143        nan 0.68285714        nan 0.68857143\n",
            "        nan 0.66571429        nan 0.69428571        nan 0.68857143\n",
            "        nan 0.69142857        nan 0.68857143        nan 0.68857143\n",
            "        nan 0.68285714        nan 0.68857143        nan 0.65714286\n",
            "        nan 0.68571429        nan 0.66              nan 0.68571429\n",
            "        nan 0.65142857        nan 0.68              nan 0.66571429\n",
            "        nan 0.68              nan 0.66571429        nan 0.69428571\n",
            "        nan 0.69714286        nan 0.69142857        nan 0.67428571\n",
            "        nan 0.68857143        nan 0.68571429        nan 0.68857143\n",
            "        nan 0.66571429        nan 0.69428571        nan 0.69428571\n",
            "        nan 0.69142857        nan 0.68571429        nan 0.68857143\n",
            "        nan 0.68571429        nan 0.68857143        nan 0.66285714\n",
            "        nan 0.68571429        nan 0.65714286        nan 0.68571429\n",
            "        nan 0.64571429        nan 0.68              nan 0.65714286\n",
            "        nan 0.68              nan 0.65428571        nan 0.69428571\n",
            "        nan 0.68857143        nan 0.69142857        nan 0.66285714\n",
            "        nan 0.68857143        nan 0.68285714        nan 0.68857143\n",
            "        nan 0.66571429        nan 0.69428571        nan 0.68857143\n",
            "        nan 0.69142857        nan 0.68857143        nan 0.68857143\n",
            "        nan 0.68285714        nan 0.68857143        nan 0.65714286\n",
            "        nan 0.68571429        nan 0.66              nan 0.68571429\n",
            "        nan 0.65142857        nan 0.68              nan 0.66571429\n",
            "        nan 0.68              nan 0.66571429        nan 0.69428571\n",
            "        nan 0.69714286        nan 0.69142857        nan 0.67428571\n",
            "        nan 0.68857143        nan 0.68571429        nan 0.68857143\n",
            "        nan 0.66571429        nan 0.69428571        nan 0.69428571\n",
            "        nan 0.69142857        nan 0.68571429        nan 0.68857143\n",
            "        nan 0.68571429        nan 0.68857143        nan 0.66285714]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter set: {'clf__alpha': 1.0, 'clf__class_prior': None, 'tcp__kw_args': {'lowercase': True}, 'vect__max_df': 0.95, 'vect__max_features': 1000, 'vect__min_df': 7, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_stemmer at 0x7fab91deb830>} \n",
            "CV Accuracy: 0.697\n",
            "Test Accuracy: 0.633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='red'>2.3 Exercise</font> Exploring text classifier alternatives with grid search\n",
        "\n",
        "Suppose we want to explore the impact of different chi squared feature selection subset sizes e.g. 50, 100, 200, 400, 700 on a NaiveBayes classifer. \n",
        "\n",
        "How can we set up such an experiment?\n",
        "Note: you can set all the others to the best params from 2.2"
      ],
      "metadata": {
        "id": "-VCoDVZk_9bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOLUTION 2.3\n",
        "param_grid23 = [{\n",
        "\n",
        "    'vect__ngram_range': [(1,1)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
        "    'vect__tokenizer': [tokenizer_stemmer], # use a tokeniser and the stemmer \n",
        "    'vect__stop_words': [(None)], # try with and without the stop dictionary\n",
        "    'vect__max_features': [(1000)], # a vocabulary that only consider the top max_features ordered by term frequency\n",
        "    'vect__min_df': [(7)], # ignore terms that have a document frequency strictly lower than )\n",
        "    'vect__max_df': [(0.95)], #ignore terms that have a document frequency strictly higher \n",
        "    'chi__k': [(50),(100), (200), (500)],\n",
        "    'tcp__kw_args': [{'lowercase': True}]  # case insensitive or not parameters for our custom transformer\n",
        "  }]\n",
        "\n",
        "#Sentiment classification with a neural net\n",
        "pipe_tfidf23 = Pipeline([\n",
        "                  ('tcp', text_cleanup_preprocessor),\n",
        "                  ('vect', TfidfVectorizer()),\n",
        "                  ('chi', SelectKBest(chi2)),\n",
        "                  ('clf',  MultinomialNB(alpha=1.0, class_prior=None))\n",
        "\n",
        "  ])\n",
        "                \n",
        "gs_sentiment_classification23 = GridSearchCV(pipe_tfidf23, param_grid23,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=1)"
      ],
      "metadata": {
        "id": "qvqKEZDqAMPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_sentiment_classification23.fit(X_train, y_train)\n",
        "print('Best parameter set: %s ' % gs_sentiment_classification23.best_params_)\n",
        "print('Best CV Accuracy: %.3f' % gs_sentiment_classification23.best_score_)\n",
        "\n",
        "clf23 = gs_sentiment_classification23.best_estimator_\n",
        "print('Best Accuracy on all Test data: %.3f' % clf23.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RekONRq5A84X",
        "outputId": "1a3e6785-b6de-4e0d-c870-3e0360296067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best parameter set: {'chi__k': 500, 'tcp__kw_args': {'lowercase': True}, 'vect__max_df': 0.95, 'vect__max_features': 1000, 'vect__min_df': 7, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_stemmer at 0x7fab91deb830>} \n",
            "Best CV Accuracy: 0.771\n",
            "Best Accuracy on all Test data: 0.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_all_results_df = pd.DataFrame(gs_sentiment_classification23.cv_results_)\n",
        "feature_selection_results = cv_all_results_df[[\"param_chi__k\",\"mean_test_score\"]]\n",
        "feature_selection_results.plot.bar(y='mean_test_score', x=\"param_chi__k\", rot=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "zxR2230LPoqk",
        "outputId": "925d196a-961a-4782-820e-9c6466a63e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fab90b019d0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y0JqzO_5WyK"
      },
      "source": [
        "# 3. Text Classification Case Studies \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyk0Of-D5WyM"
      },
      "source": [
        "## SMS spam filter\n",
        "\n",
        "SMS Spam filter as a classification task. We first load the text data \"SMSSpamCollection\".\n",
        "\n",
        "Furthermore, we perform some simple preprocessing and split the data array into two parts:\n",
        "\n",
        "1. `text`: A list of lists, where each sublists contains the contents of our emails\n",
        "2. `y`: our SPAM vs HAM labels stored in binary; a 1 represents a spam message, and a 0 represnts a ham (non-spam) message. \n",
        "1. Use the Tfidf vectoriser to transofrm the email content. \n",
        "2. Setup a gridsearch with pipelines to identify a good model for SMS message classification\n",
        "\n",
        "NOTE: the class labels column is categorical. This means that we need a class mapping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC9lGaqe5WyL"
      },
      "source": [
        "sms_filename = \"https://raw.githubusercontent.com/nirmalie/CM4107/main/SMSSpamData.csv\"\n",
        "df_sms = pd.read_csv(sms_filename) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYYSk3ut5WyM"
      },
      "source": [
        "print(\"length of the dataset:\", len(df_sms))\n",
        "df_sms.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWA8qCDMXzt2"
      },
      "source": [
        "# Get the classes mapped to 0 and 1\n",
        "class_labels = (np.unique(df_sms['class'])) # get the unique class labels\n",
        "print(\"Mapping class labels:\", class_labels)\n",
        "sms_class_mapping = {label:idx for idx,label in enumerate(class_labels)}\n",
        "\n",
        "sms_class_labels = [x for x in sms_class_mapping] # store the class labels for later\n",
        "\n",
        "df_sms[\"class\"] = df_sms[\"class\"].map(sms_class_mapping)\n",
        "df_sms.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvNWdSkmnf21"
      },
      "source": [
        "# get the data\n",
        "X = df_sms.loc[:, 'sms_msg'].values\n",
        "y = df_sms.loc[:, 'class'].values\n",
        "text_train, text_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                          random_state=42,\n",
        "                                                          test_size=0.30, # lets use 30% for testing\n",
        "                                                          stratify=y)\n",
        "\n",
        "small_df= df_sms.sample(frac=0.4, replace=False, random_state=1)\n",
        "\n",
        "# form X and y\n",
        "X = small_df.loc[:, 'sms_msg'].values\n",
        "y = small_df.loc[:, 'class'].values\n",
        "# get the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                          random_state=42,\n",
        "                                                          test_size=0.30, # lets use 30% for testing\n",
        "                                                          stratify=y)\n",
        "print(\"Sample of Training data:\", X_train.shape)\n",
        "print(\"Sample of Test data:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg4nnXfqnlya"
      },
      "source": [
        "# set the pipelines and grid search\n",
        "param_grid_sms = [{\n",
        "    'vect__ngram_range': [(1,2)], #can also extract 2-grams of words in addition to the 1-grams (individual words)\n",
        "    'vect__tokenizer': [tokenizer_stemmer], # use a tokeniser and the stemmer \n",
        "    'vect__max_features': [1000, 4000], # set the vocabulary size'\n",
        "    'vect__stop_words': [None] # use the stop dictionary of stopwords or not\n",
        "              }]\n",
        "\n",
        "#Sentiment classification with a neural net\n",
        "pipe_tfidf_sms = Pipeline([\n",
        "                  ('tcp', text_cleanup_preprocessor),       \n",
        "                  ('vect', TfidfVectorizer( min_df=7)),  \n",
        "                  ('chi', SelectKBest(chi2, k = 250)),    \n",
        "                  ('cls', SVC(kernel='rbf', gamma=0.7, C=1.0)) # SVM classifier\n",
        "                  ]) \n",
        "                \n",
        "gs_sms_classification = GridSearchCV(pipe_tfidf_sms, param_grid_sms,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRc_Ia2Y6zFM"
      },
      "source": [
        "# fit the grid search \n",
        "gs_sms_classification.fit(X_train, y_train) \n",
        "print('Best parameter set: %s ' % gs_sms_classification.best_params_)\n",
        "print('CV Accuracy: %.3f' % gs_sms_classification.best_score_)\n",
        "clf = gs_sms_classification.best_estimator_\n",
        "\n",
        "#use the test data to evaluate\n",
        "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWODI-kzMS8N"
      },
      "source": [
        "## <font color='red'>3.1 Exercise</font> Carry out text classification on the Newsgroup dataset\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
        "You can find out more about the dataset from [sklearn dataset documentation](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
        "\n",
        "Build a text classifier for this dataset by following these steps:\n",
        "1.   Load the data an examine structure \n",
        "2.   Set the pipelines\n",
        "3.   Specify the grid search\n",
        "4.   Use GridSearchCV to determine the best model and parameters\n",
        "5.   Analyse the result on the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyvwCmNg9EFF"
      },
      "source": [
        "#1. Load the data an examine structure\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pprint import pprint\n",
        "\n",
        "# please refer to sklearn documentation about the full dataset\n",
        "# here we will work on a small sample of categories instead of the 20 categories\n",
        "cats = ['sci.crypt','sci.electronics','sci.med','sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=cats)\n",
        "\n",
        "pprint(list(newsgroups_train.target_names))\n",
        "news_class_labels = list(newsgroups_train.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kutz9GVYDMoV"
      },
      "source": [
        "train_data= pd.Series(newsgroups_train.data) \n",
        "\n",
        "\n",
        "train_df=pd.DataFrame(train_data) \n",
        "train_df.columns = ['Data'] + train_df.columns.tolist()[1:] \n",
        "train_df['target'] = pd.Series(newsgroups_train.target)\n",
        "print(\"Size of the corpus:\", len(train_df))\n",
        "train_df.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWMiqLezycD0"
      },
      "source": [
        "# get the test data\n",
        "test_data= pd.Series(newsgroups_test.data) \n",
        "\n",
        "test_df=pd.DataFrame(test_data) \n",
        "test_df.columns = ['Data'] + test_df.columns.tolist()[1:] # add the 'target' to the list of columns\n",
        "test_df['target'] = pd.Series(newsgroups_test.target)\n",
        "\n",
        "#Strip HTML and punctuation to speed up text processing\n",
        "test_df['Data'] = test_df['Data'].apply(preprocessor)\n",
        "\n",
        "print(\"Size of the test data:\", len(test_df))\n",
        "test_df.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PImUO0xO89ak"
      },
      "source": [
        "#get a small sample from the 50K data\n",
        "small_df= train_df.sample(frac=0.5, replace=False, random_state=1)\n",
        "\n",
        "X_train = small_df.loc[:, 'Data'].values\n",
        "y_train = small_df.loc[:, 'target'].values\n",
        "\n",
        "\n",
        "print(\"Sample of Training data:\", X_train.shape)\n",
        "\n",
        "\n",
        "#Test data\n",
        "X_test = test_df.loc[:, 'Data'].values\n",
        "y_test = test_df.loc[:, 'target'].values\n",
        "print(\"Test data:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nyn5ucCnI8-"
      },
      "source": [
        "# 2. Set pipline\n",
        "news_pipe_tfidf = Pipeline([\n",
        "    #complete code here\n",
        "\n",
        "    ]) \n",
        "\n",
        "\n",
        "# 3. set the param grid\n",
        "news_param_grid = [{\n",
        "      #complete code here\n",
        "  }]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhJUeG7TpYjR"
      },
      "source": [
        "# 4. Fit the best model using GridSearchCV\n",
        "gs_news_classification = GridSearchCV( ??, ?? # complete here\n",
        "                          scoring='accuracy',\n",
        "                          cv=5,\n",
        "                          verbose=1,\n",
        "                          n_jobs=1)\n",
        "\n",
        "gs_news_classification.fit(X_train, y_train)\n",
        "print('Best parameter set: %s ' % gs_news_classification.best_params_)\n",
        "print('CV Accuracy: %.3f' % gs_news_classification.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YsW-UV-0_AO"
      },
      "source": [
        "# 5. Analyse the result on test data\n",
        "news_clf = ?  # model from the best estimator\n",
        "\n",
        "# detailed report on test data using classification_report\n",
        "y_pred = ?\n",
        "print(classification_report(?))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCGOLzx1vd4u"
      },
      "source": [
        "#4. Comparative study of multiple algorithms using the Sentiment 140 dataset\n",
        "\n",
        "The sentiment140 dataset contains 1,600,000 tweets extracted using the twitter API. The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment.\n",
        "\n",
        "https://www.kaggle.com/kazanova/sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXvl7aWqOuVz"
      },
      "source": [
        "# download the zipped dataset to your colab files area and unzip it\n",
        "!wget 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
        "!unzip trainingandtestdata.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5FVCiVVQNma"
      },
      "source": [
        "csv_file_name = \"training.1600000.processed.noemoticon.csv\"\n",
        "df_senti140 = pd.read_csv(csv_file_name, header=None, encoding='latin-1')\n",
        "df_senti140.columns =['target', 'id', 'date', 'flag', 'user', 'text']  # the dataset has no header so we add column names\n",
        "\n",
        "df_senti140.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JN8A2-FtFqL"
      },
      "source": [
        "df_senti140_small = df_senti140.sample(frac=0.01, replace=False, random_state=1)\n",
        "\n",
        "#get the X and y parts of data\n",
        "X = df_senti140_small.loc[:, 'text'].values\n",
        "y = df_senti140_small.loc[:, 'target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                          random_state=42,\n",
        "                                                          test_size=0.30, # lets use 30% for testing\n",
        "                                                          stratify=y)\n",
        "print(\"Shape of Training data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh0XuMR8181"
      },
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def gsearch(models, train_data, train_target, test_data, test_target, text_vectoriser, parameter_grid, scoring=\"accuracy\", folds=5):\n",
        "    \"\"\"\n",
        "    Performs grid searches for models and returns the training and test accuracies. \n",
        "    \"\"\"\n",
        "    dat = []\n",
        "    ##iterate through each model\n",
        "    for name, model, params in models:\n",
        "        # add any model parameters to the parameter grid\n",
        "        param_grid = copy.deepcopy(parameter_grid)  # deep copy so that 'parameter_grid' does not maintain a reference to 'param_grid'\n",
        "        for param_name, param_options in params:\n",
        "            param_grid[0]['model__' + param_name] = param_options\n",
        "\n",
        "        # the pipeline\n",
        "        pipe_tfidf = Pipeline([('clean', text_cleanup_preprocessor),\n",
        "                               ('vector', text_vectoriser),\n",
        "                               ('selector', SelectKBest(chi2, k = 800)),\n",
        "                               ('model',  model)]) \n",
        "        \n",
        "        # grid search cv\n",
        "        gs_sentiment_classification = GridSearchCV(pipe_tfidf, param_grid,\n",
        "                              scoring=scoring,\n",
        "                              cv=folds,\n",
        "                              verbose=1,\n",
        "                              n_jobs=-1)\n",
        "        \n",
        "        gs_sentiment_classification.fit(train_data, train_target)\n",
        "\n",
        "        print('Best parameter set: %s ' % gs_sentiment_classification.best_params_)\n",
        "        train_score = gs_sentiment_classification.best_score_\n",
        "        print('CV Accuracy: %.3f' % train_score)\n",
        "\n",
        "        clf = gs_sentiment_classification.best_estimator_\n",
        "        test_score = clf.score(test_data, test_target)\n",
        "        print('Test Accuracy: %.3f\\n' % test_score)\n",
        "        dat.append([name, train_score, test_score])\n",
        "\n",
        "    return pd.DataFrame(dat, columns=['Model', 'Train Accuracy', 'Test Accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmYzDuD4t76H"
      },
      "source": [
        "# models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(), [('max_iter', [100]), ('solver', [('lbfgs'), ('liblinear')]), ('multi_class', ['auto'])]))\n",
        "models.append(('NB', MultinomialNB(alpha=1.0, class_prior=None), []))\n",
        "models.append(('SVM', SVC(kernel='linear', gamma=0.1, C=1.0), []))\n",
        "\n",
        "preprocess_param_grid = [{'vector__ngram_range': [(1, 1)], # ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features\n",
        "               'vector__stop_words': [stop, None], # use the stop dictionary of stopwords or not\n",
        "               'vector__tokenizer': [tokenizer_stemmer]}, # use a tokeniser and the stemmer \n",
        "               ]\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "plot_data = gsearch(models, X_train, y_train, X_test, y_test, tfidf, preprocess_param_grid)\n",
        "print(plot_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S10RfcpDyTdm"
      },
      "source": [
        "# plot grouped bar chart\n",
        "plot_data.plot.bar(x='Model', ylim=(.5, 1.))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}